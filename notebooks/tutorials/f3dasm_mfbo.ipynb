{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-fidelity Bayesian optimization (MFBO) with F3DASM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Goal of this notebook\n",
    "This notebook serves to answer the following questions in order:\n",
    "1. Which Python packages do I need to import in order to perform MFBO with GPR in F3DASM, and why?\n",
    "2. What are the hyperparameters that MFBO with GPR uses and do I define them?\n",
    "3. How do I run the MFBO algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the `f3dasm` library, which serves as the framework for doing BO with GPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import f3dasm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BO pipeline in F3DASM is based on the `pytorch` framework. As such, we need to import these packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gpytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to import the following packages to do the necessary intermediate math and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the [GPR in F3DASM](./f3dasm_sogpr.ipynb) tutorial, we assume that there is a data source defined by an analytic function. However, unlike the GPR tutorial, it is now necessary to define a `MultiFidelityFunction` in order to accomodate the multi-fidelity BO.\n",
    "\n",
    "The main differences are:\n",
    "- Each of the two fidelities has an associated function. In this tutorial, the low-fidelity function is an augmented version of the high-fidelity base function.\n",
    "- Each of the two fidelity functions has an associated cost level: an assigned positive value which is incurred every time that the fidelity function in question is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 1\n",
    "fidelity_parameters = [0.5, 1.]\n",
    "costs = [0.5, 1.]\n",
    "\n",
    "fun_class = f3dasm.functions.AlpineN2\n",
    "\n",
    "base_fun = fun_class(\n",
    "    dimensionality=dimensionality,\n",
    "    scale_bounds=np.tile([0.0, 1.0], (dimensionality, 1)),\n",
    "    )\n",
    "\n",
    "###\n",
    "\n",
    "fidelity_functions = []\n",
    "\n",
    "for i, fidelity_parameter in enumerate(fidelity_parameters):\n",
    "\n",
    "    fun = f3dasm.base.function.AugmentedFunction(\n",
    "            base_fun=base_fun,\n",
    "            fid=fidelity_parameter,\n",
    "            dimensionality=base_fun.dimensionality,\n",
    "            scale_bounds=base_fun.scale_bounds,\n",
    "            )\n",
    "\n",
    "    fidelity_functions.append(fun)\n",
    "\n",
    "multifidelity_function = f3dasm.base.function.MultiFidelityFunction(\n",
    "    fidelity_functions=fidelity_functions,\n",
    "    fidelity_parameters=fidelity_parameters,\n",
    "    costs=costs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to minimize this objective function using Bayesian optimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 MFGPR hyperparameters\n",
    "In order for us to perform MFBO with GPR, we first need to specify the necessary hyperparameters of the multi-fidelity GP regression model; in this case, we use cokgj. \n",
    "\n",
    "This is done similarly in the way described in the [MFGPR tutorial](./f3dasm_mfgpr.ipynb), so the details will be omitted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_module_list = torch.nn.ModuleList([\n",
    "    gpytorch.means.ZeroMean(),\n",
    "    gpytorch.means.ZeroMean()\n",
    "])\n",
    "\n",
    "covar_module_list = torch.nn.ModuleList([\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()),\n",
    "])\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "opt_algo = torch.optim.Adam\n",
    "opt_algo_kwargs = dict(lr=0.1)\n",
    "training_iter = 50\n",
    "\n",
    "noisy_data_bool = False\n",
    "seed = 123\n",
    "\n",
    "regressor_hyperparameters = f3dasm.machinelearning.gpr.Cokgj_Parameters(\n",
    "    kernel=covar_module_list,\n",
    "    mean=mean_module_list,\n",
    "    likelihood=likelihood,\n",
    "    noise_fix=1 - noisy_data_bool,\n",
    "    opt_algo=opt_algo,\n",
    "    opt_algo_kwargs=opt_algo_kwargs,\n",
    "    training_iter=training_iter,\n",
    "    )\n",
    "\n",
    "regressor = f3dasm.machinelearning.Cokgj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MFBO hyperparameters\n",
    "\n",
    "Next, hyperparameters for the BO scheme are to be selected. These consist of two main parts:\n",
    "\n",
    "1. The regressor class and corresponding hyperparameters;\n",
    "2. The acquisition function class and corresponding hyperparameters.\n",
    "\n",
    "The regressor has been taken care of in the previous code cell. For this tutorial, we will select the variable fidelity upper confidence bound acquisition function [REFERENCE] with the appropriate hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition = f3dasm.base.acquisition.VFUpperConfidenceBound\n",
    "acquisition_hyperparameters = f3dasm.optimization.bayesianoptimization_torch.Acquisition_Parameters(\n",
    "    beta=0.4,\n",
    "    maximize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the regressor and acquisition related objects, a few other parameters are to be defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 1\n",
    "iterations = 5\n",
    "numbers_of_samples = [20, 5]\n",
    "fidelity_parameters = [0.5, 1.]\n",
    "costs = [0.5, 1.]\n",
    "budget = 10\n",
    "visualize_gp = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the two parts into the optimizer parameter object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_parameter = f3dasm.optimization.bayesianoptimization_torch.MFBayesianOptimizationTorch_Parameters(\n",
    "    regressor=regressor,\n",
    "    acquisition=acquisition,\n",
    "    regressor_hyperparameters=regressor_hyperparameters,\n",
    "    acquisition_hyperparameters=acquisition_hyperparameters,\n",
    "    visualize_gp=visualize_gp,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running MFBO\n",
    "\n",
    "Before we can initialize the optimizer, we first need to identify the space and sampling method corresponding to the optimization problem.\n",
    "\n",
    "In the case of a multi-fidelity Bayesian optimization problem, a sampler is associated with each fidelity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multifidelity_samplers = []\n",
    "\n",
    "for i in [0, 1]:\n",
    "    parameter_DesignSpace = f3dasm.make_nd_continuous_design(\n",
    "        bounds=np.tile([0.0, 1.0], (dimensionality, 1)),\n",
    "        dimensionality=dimensionality,\n",
    "    )\n",
    "\n",
    "    sampler = f3dasm.sampling.SobolSequence(design=parameter_DesignSpace, seed=seed)\n",
    "\n",
    "    multifidelity_samplers.append(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the optimizer can be defined with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = f3dasm.optimization.MFBayesianOptimizationTorch(\n",
    "    data=f3dasm.ExperimentData(design=parameter_DesignSpace),\n",
    "    multifidelity_function=multifidelity_function,\n",
    "    )\n",
    "optimizer.parameter = opt_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = f3dasm.run_multi_fidelity_optimization(\n",
    "    optimizer=optimizer,\n",
    "    multifidelity_function=multifidelity_function,\n",
    "    multifidelity_samplers=multifidelity_samplers,\n",
    "    iterations=iterations,\n",
    "    seed=seed,\n",
    "    numbers_of_samples=numbers_of_samples,\n",
    "    budget=budget\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1].data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('f3dasm_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9d896b1e26959336e7f4be8af34f758934959c162bfe59669aabbaab50cee31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
